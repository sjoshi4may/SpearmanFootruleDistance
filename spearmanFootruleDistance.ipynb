{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Spearman footrule : The sum of absolute values of the difference in the ranks.\n",
    "\n",
    "**Reference**\n",
    "1. An efficient approach for the rank aggregation problem : doi:10.1016/j.tcs.2006.05.024\n",
    "\n",
    "**Rank Aggregation Problem**\n",
    "\n",
    "1. Objects under cosideration are ordered according to several different criterion.\n",
    "\n",
    "2. One looks for a ranking that is as close as possible to or, in a broader sense, combinesâ€”the rankings obtained in the first step.\n",
    "\n",
    "\n",
    "**Implementation** \n",
    "\n",
    "1. We're given the rankings that are to be combined. We're also given the proposed ranking.\n",
    "\n",
    "2. Use spearman footrule to get the distance between the proposed ranking and the given rankings.\n",
    "\n",
    "3. The ranking that minimizes the sum of the distances from proposed ranking to all the given rankings is returned as the output of the rank aggregation problem.\n",
    "\n",
    "\n",
    "**Assumptions**\n",
    "\n",
    "1. We've a full list of ranking present i.e given 3 items we've a score for each of them aka no partial ranking.\n",
    "\n",
    "2. If there are score collisions, all the items with same score are assigned the same rank (minimum of the group) and    an appropriate number of ranks are skipped e.g\n",
    " \n",
    "Item  Score  Rank\n",
    " \n",
    " A : 100   : 1\n",
    " \n",
    " B : 100   : 1\n",
    " \n",
    " C : 100   : 1\n",
    " \n",
    " D : 65    : 4\n",
    " \n",
    " \n",
    "\n",
    "**Interesting reads**\n",
    "\n",
    "1. Kendall tau distance\n",
    "\n",
    "2. http://www10.org/cdrom/papers/pdf/p577.pdf \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unittest\n",
    "\n",
    "def scoresToRanks(scores):\n",
    "    \"\"\"returns a set of ranks based on the given scores for each measurement.\n",
    "    :param score: dictionary of tuple where key is itemId and tuple contains scores by different metrics.\n",
    "\n",
    "    :return: A pandas dataframe with ranks generated from each metric in a column.\n",
    "\n",
    "    e.g scores = {'A' : (100,0.1),\n",
    "          'B' : (90,0.3),\n",
    "          'C' : (20, 0.2)}\n",
    "\n",
    "        returns dataframe      Index     metric_0    metric_1\n",
    "                               A            1           3\n",
    "                               B            2           1\n",
    "                               C            3           2\n",
    "\n",
    "    \"\"\"\n",
    "    n_items = len(scores)\n",
    "    if n_items:     # Check if scores is non-empty\n",
    "        df = pd.DataFrame.from_dict(scores,orient='index')   # Using pandas to handle cases with huge number of entries\n",
    "        df.columns = ['metric_'+ str(col) for col in df.columns]\n",
    "        ranks = []\n",
    "        for col in df.columns:\n",
    "            df.sort_values(by=col,ascending=False,inplace=True) # Sort the index by the selected column\n",
    "            df['rank'] = range(1,df.shape[0]+1) # Assign the ranks in descending order\n",
    "            \n",
    "            if df[col].nunique() != df.shape[0]: # check if all the scores are different i.e no collisions\n",
    "                df[col] = df.groupby(col)['rank'].transform('min')\n",
    "            else:\n",
    "                df[col] = range(1,df.shape[0]+1)\n",
    "                           \n",
    "            df.drop('rank',axis=1,inplace=True)\n",
    "    else:\n",
    "        raise Exception(\"Sorry, no scores are provided.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def sumSpearmanDistances(scores, proposedRank):\n",
    "    \"\"\"Calculates spearman footrule distance between the \n",
    "        proposed rank and the multiset of ranks generated by scores.\n",
    "        \n",
    "        :param scores: \n",
    "        :param proposedRank:\n",
    "        \n",
    "        :return:\n",
    "        \n",
    "        e.g ranks = [{'A':1, 'B':2, 'C':3},{'A':3, 'B':1, 'C':2}]\n",
    "        \n",
    "        proposedRank = {'A':1, 'B':2, 'C':3}  --> returns (0,4)\n",
    "        proposedRank = {'A':2, 'B':3, 'C':1}  --> returns (4,4)\n",
    "        proposedRank = {'A':3, 'B':1, 'C':2}  --> returns (4,0)\n",
    "        proposedRank = {'A':1, 'B':3, 'C':2}  --> returns (2,4)\n",
    "        proposedRank = {'A':2, 'B':1, 'C':3}  --> returns (2,2)\n",
    "        proposedRank = {'A':3, 'B':2, 'C':1}  --> returns (4,2)\n",
    "        \"\"\"\n",
    "\n",
    "    # ToDo : Add checks for partial rank cases and partial metric values being present.\n",
    "    n_items = len(proposedRank)\n",
    "    if n_items:\n",
    "        ranks = scoresToRanks(scores) # Convert scores to ranks\n",
    "        if not ranks.empty:\n",
    "            ranks = ranks.reindex(proposedRank) # Reshuffle the dataset according to the proposed rank\n",
    "            ranks['proposed_rank'] = range(1,ranks.shape[0]+1) # Assign proposed ranks\n",
    "    \n",
    "            distances = 0 \n",
    "            for col in ranks.columns:\n",
    "                if col != 'proposed_rank':\n",
    "                    distances+=(ranks[col]-ranks['proposed_rank']).abs().sum() # calculate spearman footrule distance\n",
    "            return distances    \n",
    " \n",
    "    raise Exception(\"Sorry, proposed rank is not provided\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSumSpearmanDistances(unittest.TestCase):\n",
    "\n",
    "    def test_case1(self):\n",
    "        scores = {'A' : (100,0.1),'B' : (90,0.3),'C' : (20, 0.2)}\n",
    "        proposedRank = ['A', 'B', 'C']\n",
    "        self.assertEqual(sumSpearmanDistances(scores,proposedRank), 4)\n",
    "\n",
    "    def test_case2(self):\n",
    "        scores = {'A' : (100,0.1),'B' : (90,0.3),'C' : (20, 0.2)}\n",
    "        proposedRank = ['C', 'A', 'B']\n",
    "        self.assertEqual(sumSpearmanDistances(scores,proposedRank), 8)\n",
    "\n",
    "    def test_case3(self):\n",
    "        scores = {'A' : (100,0.1),'B' : (90,0.3),'C' : (20, 0.2)}\n",
    "        proposedRank = ['A', 'C', 'B']\n",
    "        self.assertEqual(sumSpearmanDistances(scores,proposedRank), 6)\n",
    "    \n",
    "    def test_case4(self):\n",
    "        scores = {'A' : (100,0.1),'B' : (90,0.3),'C' : (20, 0.2)}\n",
    "        proposedRank = ['C', 'B', 'A']\n",
    "        self.assertEqual(sumSpearmanDistances(scores,proposedRank), 6)\n",
    "    \n",
    "    def test_case5(self):\n",
    "        scores = {'A' : (100,0.1),'B' : (90,0.3),'C' : (20, 0.2)}\n",
    "        proposedRank = ['B', 'A', 'C']\n",
    "        self.assertEqual(sumSpearmanDistances(scores,proposedRank), 4)\n",
    "    \n",
    "    def test_case6(self):\n",
    "        scores = {'A' : (100,0.1),'B' : (90,0.3),'C' : (20, 0.2)}\n",
    "        proposedRank = ['B', 'C', 'A']\n",
    "        self.assertEqual(sumSpearmanDistances(scores,proposedRank), 4)\n",
    "        \n",
    "    def test_rank_collisions(self):\n",
    "        scores = {'A' : (100,0.1), 'B' : (90,0.3), 'C' : (20, 0.2),\n",
    "                 'D' : (100,0.1), 'E' : (90,0.3)}\n",
    "        proposedRank = ['A','B','C','D','E']\n",
    "        self.assertEqual(sumSpearmanDistances(scores,proposedRank), 16)\n",
    "    \n",
    "    def test_raise_proposed_rank(self):\n",
    "        scores = {'A' : (100,0.1),'B' : (90,0.3),'C' : (20, 0.2)}\n",
    "        proposedRank = []\n",
    "        with self.assertRaises(Exception) as err:\n",
    "            sumSpearmanDistances(scores,proposedRank)\n",
    "            self.assertTrue(\"Sorry, proposed rank is not provided\" in err.exception.value)\n",
    "\n",
    "    def test_raise_scores(self):\n",
    "        scores = {}\n",
    "        proposedRank = ['A','B','C']\n",
    "        with self.assertRaises(Exception) as err:\n",
    "            sumSpearmanDistances(scores,proposedRank)\n",
    "            self.assertTrue(\"Sorry, no scores are provided.\" in err.exception.value)\n",
    "            \n",
    "unittest.main(argv=['ignored', '-v'], exit=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
